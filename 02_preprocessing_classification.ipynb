{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382451a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "dossier = \"CorpusARIA/TXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a41322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event-200.txt: 48 lemmes distincts\n",
      "event-117.txt: 60 lemmes distincts\n",
      "event-76.txt: 36 lemmes distincts\n",
      "event-207.txt: 25 lemmes distincts\n",
      "event-110.txt: 111 lemmes distincts\n",
      "event-71.txt: 144 lemmes distincts\n",
      "event-275.txt: 36 lemmes distincts\n",
      "event-162.txt: 66 lemmes distincts\n",
      "event-209.txt: 49 lemmes distincts\n",
      "event-119.txt: 94 lemmes distincts\n",
      "event-78.txt: 74 lemmes distincts\n",
      "event-272.txt: 38 lemmes distincts\n",
      "event-165.txt: 48 lemmes distincts\n",
      "event-31.txt: 124 lemmes distincts\n",
      "event-150.txt: 73 lemmes distincts\n",
      "event-181.txt: 47 lemmes distincts\n",
      "event-247.txt: 43 lemmes distincts\n",
      "event-36.txt: 62 lemmes distincts\n",
      "event-157.txt: 55 lemmes distincts\n",
      "event-186.txt: 33 lemmes distincts\n",
      "event-240.txt: 61 lemmes distincts\n",
      "event-38.txt: 77 lemmes distincts\n",
      "event-159.txt: 88 lemmes distincts\n",
      "event-188.txt: 34 lemmes distincts\n",
      "event-44.txt: 185 lemmes distincts\n",
      "event-125.txt: 81 lemmes distincts\n",
      "event-95.txt: 161 lemmes distincts\n",
      "event-232.txt: 55 lemmes distincts\n",
      "event-43.txt: 76 lemmes distincts\n",
      "event-122.txt: 124 lemmes distincts\n",
      "event-235.txt: 57 lemmes distincts\n",
      "event-92.txt: 127 lemmes distincts\n",
      "event-249.txt: 41 lemmes distincts\n",
      "event-2.txt: 67 lemmes distincts\n",
      "event-83.txt: 71 lemmes distincts\n",
      "event-224.txt: 49 lemmes distincts\n",
      "event-52.txt: 49 lemmes distincts\n",
      "event-133.txt: 60 lemmes distincts\n",
      "event-258.txt: 45 lemmes distincts\n",
      "event-199.txt: 37 lemmes distincts\n",
      "event-29.txt: 107 lemmes distincts\n",
      "event-148.txt: 39 lemmes distincts\n",
      "event-5.txt: 158 lemmes distincts\n",
      "event-223.txt: 47 lemmes distincts\n",
      "event-84.txt: 73 lemmes distincts\n",
      "event-55.txt: 166 lemmes distincts\n",
      "event-134.txt: 93 lemmes distincts\n",
      "event-197.txt: 45 lemmes distincts\n",
      "event-251.txt: 39 lemmes distincts\n",
      "event-27.txt: 112 lemmes distincts\n",
      "event-280.txt: 27 lemmes distincts\n",
      "event-146.txt: 60 lemmes distincts\n",
      "event-190.txt: 95 lemmes distincts\n",
      "event-256.txt: 50 lemmes distincts\n",
      "event-20.txt: 106 lemmes distincts\n",
      "event-141.txt: 98 lemmes distincts\n",
      "event-108.txt: 94 lemmes distincts\n",
      "event-69.txt: 55 lemmes distincts\n",
      "event-174.txt: 92 lemmes distincts\n",
      "event-15.txt: 131 lemmes distincts\n",
      "event-263.txt: 51 lemmes distincts\n",
      "event-173.txt: 41 lemmes distincts\n",
      "event-12.txt: 104 lemmes distincts\n",
      "event-264.txt: 48 lemmes distincts\n",
      "event-218.txt: 45 lemmes distincts\n",
      "event-101.txt: 87 lemmes distincts\n",
      "event-60.txt: 180 lemmes distincts\n",
      "event-216.txt: 51 lemmes distincts\n",
      "event-106.txt: 173 lemmes distincts\n",
      "event-67.txt: 51 lemmes distincts\n",
      "event-211.txt: 32 lemmes distincts\n",
      "event-42.txt: 109 lemmes distincts\n",
      "event-123.txt: 95 lemmes distincts\n",
      "event-93.txt: 93 lemmes distincts\n",
      "event-234.txt: 74 lemmes distincts\n",
      "event-248.txt: 25 lemmes distincts\n",
      "event-39.txt: 59 lemmes distincts\n",
      "event-158.txt: 31 lemmes distincts\n",
      "event-189.txt: 74 lemmes distincts\n",
      "event-45.txt: 58 lemmes distincts\n",
      "event-124.txt: 63 lemmes distincts\n",
      "event-233.txt: 83 lemmes distincts\n",
      "event-94.txt: 61 lemmes distincts\n",
      "event-37.txt: 73 lemmes distincts\n",
      "event-156.txt: 96 lemmes distincts\n",
      "event-187.txt: 46 lemmes distincts\n",
      "event-241.txt: 40 lemmes distincts\n",
      "event-30.txt: 79 lemmes distincts\n",
      "event-151.txt: 55 lemmes distincts\n",
      "event-180.txt: 80 lemmes distincts\n",
      "event-246.txt: 65 lemmes distincts\n",
      "event-118.txt: 60 lemmes distincts\n",
      "event-79.txt: 70 lemmes distincts\n",
      "event-273.txt: 19 lemmes distincts\n",
      "event-164.txt: 45 lemmes distincts\n",
      "event-274.txt: 47 lemmes distincts\n",
      "event-163.txt: 86 lemmes distincts\n",
      "event-208.txt: 30 lemmes distincts\n",
      "event-206.txt: 41 lemmes distincts\n",
      "event-111.txt: 75 lemmes distincts\n",
      "event-70.txt: 88 lemmes distincts\n",
      "event-201.txt: 42 lemmes distincts\n",
      "event-116.txt: 49 lemmes distincts\n",
      "event-77.txt: 89 lemmes distincts\n",
      "event-107.txt: 118 lemmes distincts\n",
      "event-66.txt: 47 lemmes distincts\n",
      "event-210.txt: 45 lemmes distincts\n",
      "event-100.txt: 69 lemmes distincts\n",
      "event-61.txt: 145 lemmes distincts\n",
      "event-217.txt: 71 lemmes distincts\n",
      "event-172.txt: 33 lemmes distincts\n",
      "event-13.txt: 155 lemmes distincts\n",
      "event-265.txt: 33 lemmes distincts\n",
      "event-219.txt: 56 lemmes distincts\n",
      "event-109.txt: 87 lemmes distincts\n",
      "event-68.txt: 56 lemmes distincts\n",
      "event-175.txt: 43 lemmes distincts\n",
      "event-14.txt: 132 lemmes distincts\n",
      "event-262.txt: 44 lemmes distincts\n",
      "event-191.txt: 50 lemmes distincts\n",
      "event-257.txt: 42 lemmes distincts\n",
      "event-21.txt: 86 lemmes distincts\n",
      "event-140.txt: 49 lemmes distincts\n",
      "event-196.txt: 45 lemmes distincts\n",
      "event-250.txt: 40 lemmes distincts\n",
      "event-281.txt: 55 lemmes distincts\n",
      "event-26.txt: 86 lemmes distincts\n",
      "event-147.txt: 74 lemmes distincts\n",
      "event-198.txt: 64 lemmes distincts\n",
      "event-28.txt: 165 lemmes distincts\n",
      "event-149.txt: 65 lemmes distincts\n",
      "event-4.txt: 125 lemmes distincts\n",
      "event-85.txt: 63 lemmes distincts\n",
      "event-222.txt: 64 lemmes distincts\n",
      "event-54.txt: 86 lemmes distincts\n",
      "event-135.txt: 93 lemmes distincts\n",
      "event-3.txt: 162 lemmes distincts\n",
      "event-225.txt: 75 lemmes distincts\n",
      "event-82.txt: 57 lemmes distincts\n",
      "event-53.txt: 77 lemmes distincts\n",
      "event-132.txt: 320 lemmes distincts\n",
      "event-259.txt: 43 lemmes distincts\n",
      "event-268.txt: 42 lemmes distincts\n",
      "event-62.txt: 170 lemmes distincts\n",
      "event-103.txt: 58 lemmes distincts\n",
      "event-214.txt: 24 lemmes distincts\n",
      "event-65.txt: 49 lemmes distincts\n",
      "event-104.txt: 157 lemmes distincts\n",
      "event-213.txt: 51 lemmes distincts\n",
      "event-19.txt: 107 lemmes distincts\n",
      "event-178.txt: 110 lemmes distincts\n",
      "event-17.txt: 127 lemmes distincts\n",
      "event-176.txt: 51 lemmes distincts\n",
      "event-261.txt: 36 lemmes distincts\n",
      "event-10.txt: 179 lemmes distincts\n",
      "event-171.txt: 37 lemmes distincts\n",
      "event-266.txt: 32 lemmes distincts\n",
      "event-253.txt: 45 lemmes distincts\n",
      "event-195.txt: 27 lemmes distincts\n",
      "event-144.txt: 30 lemmes distincts\n",
      "event-282.txt: 16 lemmes distincts\n",
      "event-25.txt: 69 lemmes distincts\n",
      "event-88.txt: 125 lemmes distincts\n",
      "event-9.txt: 80 lemmes distincts\n",
      "event-138.txt: 64 lemmes distincts\n",
      "event-59.txt: 139 lemmes distincts\n",
      "event-228.txt: 52 lemmes distincts\n",
      "event-254.txt: 49 lemmes distincts\n",
      "event-192.txt: 20 lemmes distincts\n",
      "event-143.txt: 107 lemmes distincts\n",
      "event-22.txt: 101 lemmes distincts\n",
      "event-226.txt: 44 lemmes distincts\n",
      "event-81.txt: 66 lemmes distincts\n",
      "event-0.txt: 157 lemmes distincts\n",
      "event-131.txt: 84 lemmes distincts\n",
      "event-50.txt: 47 lemmes distincts\n",
      "event-86.txt: 74 lemmes distincts\n",
      "event-221.txt: 47 lemmes distincts\n",
      "event-7.txt: 193 lemmes distincts\n",
      "event-136.txt: 66 lemmes distincts\n",
      "event-57.txt: 184 lemmes distincts\n",
      "event-127.txt: 103 lemmes distincts\n",
      "event-46.txt: 77 lemmes distincts\n",
      "event-230.txt: 61 lemmes distincts\n",
      "event-97.txt: 97 lemmes distincts\n",
      "event-120.txt: 93 lemmes distincts\n",
      "event-41.txt: 82 lemmes distincts\n",
      "event-90.txt: 146 lemmes distincts\n",
      "event-237.txt: 36 lemmes distincts\n",
      "event-239.txt: 74 lemmes distincts\n",
      "event-152.txt: 70 lemmes distincts\n",
      "event-33.txt: 105 lemmes distincts\n",
      "event-245.txt: 65 lemmes distincts\n",
      "event-183.txt: 48 lemmes distincts\n",
      "event-155.txt: 153 lemmes distincts\n",
      "event-34.txt: 100 lemmes distincts\n",
      "event-242.txt: 61 lemmes distincts\n",
      "event-184.txt: 61 lemmes distincts\n",
      "event-129.txt: 118 lemmes distincts\n",
      "event-48.txt: 67 lemmes distincts\n",
      "event-99.txt: 128 lemmes distincts\n",
      "event-277.txt: 24 lemmes distincts\n",
      "event-160.txt: 23 lemmes distincts\n",
      "event-270.txt: 12 lemmes distincts\n",
      "event-167.txt: 45 lemmes distincts\n",
      "event-202.txt: 30 lemmes distincts\n",
      "event-74.txt: 133 lemmes distincts\n",
      "event-115.txt: 80 lemmes distincts\n",
      "event-169.txt: 56 lemmes distincts\n",
      "event-279.txt: 40 lemmes distincts\n",
      "event-205.txt: 33 lemmes distincts\n",
      "event-73.txt: 59 lemmes distincts\n",
      "event-112.txt: 85 lemmes distincts\n",
      "event-220.txt: 42 lemmes distincts\n",
      "event-87.txt: 151 lemmes distincts\n",
      "event-6.txt: 204 lemmes distincts\n",
      "event-137.txt: 62 lemmes distincts\n",
      "event-56.txt: 107 lemmes distincts\n",
      "event-80.txt: 67 lemmes distincts\n",
      "event-227.txt: 32 lemmes distincts\n",
      "event-1.txt: 155 lemmes distincts\n",
      "event-130.txt: 94 lemmes distincts\n",
      "event-51.txt: 42 lemmes distincts\n",
      "event-229.txt: 55 lemmes distincts\n",
      "event-255.txt: 51 lemmes distincts\n",
      "event-193.txt: 36 lemmes distincts\n",
      "event-142.txt: 81 lemmes distincts\n",
      "event-284.txt: 30 lemmes distincts\n",
      "event-23.txt: 98 lemmes distincts\n",
      "event-252.txt: 36 lemmes distincts\n",
      "event-194.txt: 40 lemmes distincts\n",
      "event-145.txt: 85 lemmes distincts\n",
      "event-24.txt: 171 lemmes distincts\n",
      "event-283.txt: 45 lemmes distincts\n",
      "event-89.txt: 125 lemmes distincts\n",
      "event-8.txt: 113 lemmes distincts\n",
      "event-139.txt: 79 lemmes distincts\n",
      "event-58.txt: 195 lemmes distincts\n",
      "event-11.txt: 65 lemmes distincts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event-170.txt: 50 lemmes distincts\n",
      "event-267.txt: 55 lemmes distincts\n",
      "event-16.txt: 125 lemmes distincts\n",
      "event-177.txt: 73 lemmes distincts\n",
      "event-260.txt: 51 lemmes distincts\n",
      "event-64.txt: 161 lemmes distincts\n",
      "event-105.txt: 51 lemmes distincts\n",
      "event-212.txt: 35 lemmes distincts\n",
      "event-18.txt: 94 lemmes distincts\n",
      "event-179.txt: 65 lemmes distincts\n",
      "event-269.txt: 42 lemmes distincts\n",
      "event-63.txt: 125 lemmes distincts\n",
      "event-102.txt: 78 lemmes distincts\n",
      "event-215.txt: 66 lemmes distincts\n",
      "event-278.txt: 56 lemmes distincts\n",
      "event-204.txt: 44 lemmes distincts\n",
      "event-72.txt: 98 lemmes distincts\n",
      "event-113.txt: 72 lemmes distincts\n",
      "event-203.txt: 56 lemmes distincts\n",
      "event-75.txt: 31 lemmes distincts\n",
      "event-114.txt: 56 lemmes distincts\n",
      "event-168.txt: 50 lemmes distincts\n",
      "event-271.txt: 33 lemmes distincts\n",
      "event-166.txt: 46 lemmes distincts\n",
      "event-276.txt: 41 lemmes distincts\n",
      "event-161.txt: 57 lemmes distincts\n",
      "event-154.txt: 67 lemmes distincts\n",
      "event-35.txt: 203 lemmes distincts\n",
      "event-243.txt: 86 lemmes distincts\n",
      "event-185.txt: 63 lemmes distincts\n",
      "event-128.txt: 44 lemmes distincts\n",
      "event-49.txt: 88 lemmes distincts\n",
      "event-98.txt: 77 lemmes distincts\n",
      "event-238.txt: 91 lemmes distincts\n",
      "event-153.txt: 66 lemmes distincts\n",
      "event-32.txt: 87 lemmes distincts\n",
      "event-244.txt: 88 lemmes distincts\n",
      "event-182.txt: 60 lemmes distincts\n",
      "event-121.txt: 120 lemmes distincts\n",
      "event-40.txt: 104 lemmes distincts\n",
      "event-236.txt: 105 lemmes distincts\n",
      "event-91.txt: 73 lemmes distincts\n",
      "event-126.txt: 73 lemmes distincts\n",
      "event-47.txt: 80 lemmes distincts\n",
      "event-96.txt: 62 lemmes distincts\n",
      "event-231.txt: 56 lemmes distincts\n",
      "\n",
      "Taille totale du vocabulaire (lemmes uniques) : 2942\n"
     ]
    }
   ],
   "source": [
    "#etiquetage morpho syntaxique et lemmatisation\n",
    "\n",
    "def analyse_fichier(path):\n",
    "    text = open(path, encoding=\"utf-8\").read()\n",
    "    doc = nlp(text)\n",
    "    infos = [(t.text, t.lemma_, t.pos_) for t in doc if not t.is_punct]\n",
    "    return infos\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_lemmes = set()\n",
    "    fichiers = os.listdir(dossier)\n",
    "    n_fichiers = len(fichiers)\n",
    "\n",
    "    for f in fichiers:\n",
    "        path = os.path.join(dossier, f)\n",
    "        infos = analyse_fichier(path)\n",
    "        lemmes = {lem.lower() for (_, lem, pos) in infos if lem.isalpha()}\n",
    "        total_lemmes.update(lemmes)\n",
    "        print(f\"{f}: {len(lemmes)} lemmes distincts\")\n",
    "\n",
    "    print(f\"\\nTaille totale du vocabulaire (lemmes uniques) : {len(total_lemmes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee4c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impact          → impact          (NOUN)\n",
      "de              → de              (ADP)\n",
      "la              → le              (DET)\n",
      "foudre          → foudre          (VERB)\n",
      "sur             → sur             (ADP)\n",
      "une             → un              (DET)\n",
      "éolienne        → éolienne        (NOUN)\n",
      "\n",
      "               → \n",
      "               (SPACE)\n",
      "Dans            → dans            (ADP)\n",
      "la              → le              (DET)\n"
     ]
    }
   ],
   "source": [
    "#10 premiers mots analysés d'un fichier\n",
    "\n",
    "path = os.path.join(dossier, fichiers[0])\n",
    "for mot, lem, pos in analyse_fichier(path)[:10]:\n",
    "    print(f\"{mot:<15} → {lem:<15} ({pos})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction des lemmes pour chaque texte (pour preparer la classification)\n",
    "\n",
    "texts_lemmatized = []\n",
    "for f in fichiers:\n",
    "    path = os.path.join(dossier, f)\n",
    "    text = open(path, encoding=\"utf-8\").read()\n",
    "    doc = nlp(text)\n",
    "    lemmes = [t.lemma_.lower() for t in doc if t.is_alpha]\n",
    "    texts_lemmatized.append(\" \".join(lemmes))\n",
    "\n",
    "print(f\"{len(texts_lemmatized)} textes lemmatisés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af933544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 étiquettes chargées.\n"
     ]
    }
   ],
   "source": [
    "#on load les labels \n",
    "\n",
    "def lire_consequence_gros_grain(path_meta):\n",
    "    with open(path_meta, encoding=\"utf-8\") as f:\n",
    "        ligne = f.readline().strip()\n",
    "        champs = ligne.split(\";\")\n",
    "        if len(champs) >= 16:\n",
    "            return champs[15].strip()\n",
    "        return None\n",
    "\n",
    "labels = []\n",
    "meta_dir = \"CorpusARIA/METADATA3\"\n",
    "\n",
    "for meta in os.listdir(meta_dir):\n",
    "    path_meta = os.path.join(meta_dir, meta)\n",
    "    label = lire_consequence_gros_grain(path_meta)\n",
    "    if label:\n",
    "        labels.append(label)\n",
    "\n",
    "print(f\"{len(labels)} étiquettes chargées.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f336b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 textes lemmatisés chargés depuis Flemm.\n"
     ]
    }
   ],
   "source": [
    "dossier_flemm = \"CorpusARIA/FLEMM\"\n",
    "texts_lemmatized = []\n",
    "\n",
    "fichiers = sorted(os.listdir(dossier_flemm))  # même ordre que dans TXT\n",
    "\n",
    "for f in fichiers:\n",
    "    if f.endswith(\".flemm\"):\n",
    "        path = os.path.join(dossier_flemm, f)\n",
    "        with open(path, encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            texts_lemmatized.append(text)\n",
    "\n",
    "print(f\"{len(texts_lemmatized)} textes lemmatisés chargés depuis Flemm.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0883355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions : (285, 5000)\n",
      "Type : <class 'scipy.sparse._csr.csr_matrix'>\n",
      "matrice creuse ? True\n"
     ]
    }
   ],
   "source": [
    "#vectorization\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "#vect = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "X = vectorizer.fit_transform(texts_lemmatized)\n",
    "\n",
    "\n",
    "print(\"Dimensions :\", X.shape)\n",
    "print(\"Type :\", type(X))\n",
    "print(\"matrice creuse ?\", sparse.issparse(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36fc1180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (5-fold) : nan ± nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 1165, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 1198, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 1110, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "# validation croisée \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(vectorizer, X, labels, cv=cv, scoring='f1_macro')\n",
    "print(\"F1-macro (5-fold) : {:.2f} ± {:.2f}\".format(scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05c67019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                               precision    recall  f1-score   support\n",
      "\n",
      "                                               CONSÉQUENCES ENVIRONNEMENTALES       0.36      0.30      0.33        27\n",
      "                                                     CONSÉQUENCES ÉCONOMIQUES       0.18      0.29      0.22        14\n",
      "                      CONSÉQUENCES ÉCONOMIQUES,CONSÉQUENCES ENVIRONNEMENTALES       0.00      0.00      0.00         1\n",
      "                               CONSÉQUENCES ÉCONOMIQUES,CONSÉQUENCES SOCIALES       0.60      0.33      0.43         9\n",
      "CONSÉQUENCES ÉCONOMIQUES,CONSÉQUENCES SOCIALES,CONSÉQUENCES ENVIRONNEMENTALES       0.00      0.00      0.00         0\n",
      "                                                                     Inconnue       0.00      0.00      0.00         6\n",
      "\n",
      "                                                                     accuracy                           0.26        57\n",
      "                                                                    macro avg       0.19      0.15      0.16        57\n",
      "                                                                 weighted avg       0.31      0.26      0.28        57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#classficiation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
